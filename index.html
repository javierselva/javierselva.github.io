<html>
<head>
    <title>Javier Selva's personal site</title>
</head>

<body>
    <center><h1>Javier Selva</h1>
        <img src="./img/me.png" style="border-radius:50%;"> 
    <h3>PhD Student at Universitat de Barcelona</h3>
    <table border=0>
        <tr>
            <td><a href="https://twitter.com/javier_selvac"><img src="./img/twitter-logo.png" title="Twitter" width="80%"></a></td>
            <td><a href="https://github.com/javierselva"><img src="./img/github-logo.png" title="Github" width="80%"></a></td>
            <td><a href="https://www.linkedin.com/in/javier-selva-castell%C3%B3-399738134/"><img src="./img/linkedin-logo.png" title="Linkedin" width="80%"></a></td>
            <td><a href="https://scholar.google.com/citations?user=T5-DYyUAAAAJ"><img src="./img/scholar-logo.png" title="Google Scholar" width="80%"></a></td>
        </tr>
    </table>
</br> 
Currently doing my PhD on Learning Video representations at </br> <a href="http://www.cvc.uab.es/?page_id=223" target="blank">HuPBA</a> lab under <a href="https://sergioescalera.com/" target="blank">Sergio Escalera</a>'s supervision.


<table width="1000" border="0">
    <tr><td><h3>Publications</h3></td><td></td></tr>
    <tr>
        <td valign="top"><img src="./img/papers/dyadformer.png"></td>
        <td valign="top">(ICCV 2021 - DYAD Workshop) <font size="4"><b>Dyadformer: A Multi-modal Transformer forLong-Range Modeling of Dyadic Interactions</b></font></br> 
            David Curto*, Albert Clapés*, <b>Javier Selva</b>*, Sorina Smeureanu, Julio C. S. Jacques Junior, David Gallardo-Pujol, Georgina Guilera, David Leiva, Thomas B. Moeslund, Sergio Escalera and Cristina Palmero</br> 
            <i>We present the Dyadformer, a novel multi-modal multi-subject Transformer architecture to model individual and interpersonal features in dyadic interactions using variable time windows, thus allowing the capture of long-term interdependencies.   Our proposed cross-subject layer  allows  the  network  to  explicitly  model  interactions among subjects through attentional operations. This proof-of-concept  approach  shows  how  multi-modality  and  joint modeling  of  both  interactants  for  longer  periods  of  time helps to predict individual attributes.</i> </br> 
        </td>
    </tr>
    <tr bgcolor="#dddddd"><td></td><td></td></tr>
    <tr>
        <td valign="top"><img src="./img/papers/udiva.png"></td>
        <td valign="top">(WACV 2021 - HBU Workshop) <font size="4"><b>Context-Aware Personality Inference in Dyadic Scenarios: Introducing the UDIVA Dataset</b></font></br> 
            Cristina Palmero∗, <b>Javier Selva</b>∗, Sorina Smeureanu∗, Julio C. S. Jacques Junior, Albert Clapés, Alexa Moseguí, Zejian Zhang, David Gallardo-Pujol, Georgina Guilera, David Leiva and Sergio Escalera</br> 
            <i>This paper introduces UDIVA, a new non-acted dataset of face-to-face dyadic interactions, where interlocutors perform competitive and collaborative tasks with different behavior elicitation and cognitive workload. The dataset consists of 90.5 hours of dyadic interactions among 147 participants distributed in 188 sessions, recorded using multiple audiovisual and physiological sensors. Currently, it includes sociodemographic, self- and peer-reported personality, internal state, and relationship profiling from participants</i> </br> 
            [<a href="https://openaccess.thecvf.com/content/WACV2021W/HBU/papers/Palmero_Context-Aware_Personality_Inference_in_Dyadic_Scenarios_Introducing_the_UDIVA_Dataset_WACVW_2021_paper.pdf">PDF</a>] [<a href="https://arxiv.org/pdf/2012.14259.pdf">arXiv</a>] [<a href="https://chalearnlap.cvc.uab.cat/dataset/39/description/">Website</a>] 
            <details><summary>[Bibtex]</summary>
            <div style="background-color:#EEEEEE;">
                @inproceedings{palmero2021context,</br>
                  &emsp;title={Context-Aware Personality Inference in Dyadic Scenarios: Introducing the UDIVA Dataset},</br>
                  &emsp;author={Palmero, Cristina and Selva, Javier and Smeureanu, Sorina and Junior, Julio CS Jacques and Clap{\'e}s, Albert and Mosegu{\'\i}, Alexa and Zhang, Zejian and Gallardo-Pujol, David and Guilera, Georgina and Leiva, David and Escalera, Sergio},</br>
                  &emsp;booktitle={2021 IEEE Winter Conference on Applications of Computer Vision Workshops (WACVW)},</br>
                  &emsp;pages={1--12},</br>
                  &emsp;year={2021},</br>
                  &emsp;organization={IEEE}</br>
                }
            </div>

            </details>
        </td>
    </tr>
    <tr bgcolor="#dddddd"><td></td><td></td></tr>
    <tr>
        <td valign="top"><img src="./img/papers/recurrent-gaze.png"></td>
        <td valign="top">(BMVC 2018) <font size="4"><b>Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues</b></font></br> 
            Cristina Palmero, <b>Javier Selva</b>, Mohammad Ali Bagheri and Sergio Escalera</br> 
            <i>In this paper, we tackle the problem of person- and head pose-independent 3D gaze estimation from remote cameras, using a multi-modal recurrent convolutional neural network (CNN). We propose to combine face, eyes region, and face landmarks as individual streams in a CNN to estimate gaze in still images. Then, we exploit the dynamic nature of gaze by feeding the learned features of all the frames in a sequence to a many-to-one recurrent module that predicts the 3D gaze vector of the last frame.</i> </br> 
            [<a href="http://bmvc2018.org/contents/papers/0871.pdf">PDF</a>] [<a href="https://arxiv.org/pdf/1805.03064.pdf">arXiv</a>] [<a href="https://github.com/crisie/RecurrentGaze">Code</a>] 
            <details><summary>[Bibtex]</summary>
            <div style="background-color:#EEEEEE;">
                @inproceedings{palmero2018recurrent,</br>
                  &emsp;title={Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues},</br>
                  &emsp;author={Palmero, Cristina and Selva, Javier and Bagheri, Mohammad Ali and Escalera, Sergio},</br>
                  &emsp;booktitle={Proceedings of the British Machine Vision Conference (BMVC)},</br>
                  &emsp;year={2018}</br>
                }
            </div>

            </details>
        </td>
    </tr>
    <tr bgcolor="#dddddd"><td></td><td></td></tr>
    <tr>
        <td valign="top"><img src="./img/papers/frnn.png"></td>
        <td valign="top">(ECCV 2018) <font size="4"><b>Folded Recurrent Neural Networks for Future Video Prediction</b></font></br> 
            Marc Oliu, <b>Javier Selva</b>, and Sergio Escalera</br> 
            <i>This work introduces double-mapping Gated Recurrent Units (dGRU), an extension of standard GRUs where the input is considered as a recurrent state. An extra set of logic gates is added to update the input given the output. Stacking multiple such layers results in a recurrent auto-encoder: the operators updating the outputs comprise the encoder, while the ones updating the inputs form the decoder. Since the states are shared between corresponding encoder and decoder layers, the representation is stratified during learning: some information is not passed to the next layers.</i> </br> 
            [<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Marc_Oliu_Folded_Recurrent_Neural_ECCV_2018_paper.pdf">PDF</a>] [<a href="https://arxiv.org/abs/1712.00311">arXiv</a>] [<a href="https://github.com/moliusimon/frnn">Code</a>] 
            <details><summary>[Bibtex]</summary>
            <div style="background-color:#EEEEEE;">
                @inproceedings{oliu2018folded,</br> 
                  &emsp;title={Folded recurrent neural networks for future video prediction},</br> 
                  &emsp;author={Oliu, Marc and Selva, Javier and Escalera, Sergio},</br> 
                  &emsp;booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},</br> 
                  &emsp;pages={716--731},</br> 
                  &emsp;year={2018}</br> 
                }
            </div>

            </details>
        </td>
    </tr>
</table>
</br> 
</br> 
</br> 
</br> 
</center>
<!-- New publication template
    &emsp; to tabulate
    <tr>
        <td valign="top"><img src="./img/papers/IMAGE.png"></td>
        <td valign="top">(CONFERENCE AND YEAR) <font size="4"><b>TITLE</b></font></br> 
            AUTHORS</br> 
            <i>MINI ABSTRACT</i> </br> 
            [<a href="OFFICIAL LINK">PDF</a>] [<a href="ARXIV LINK">arXiv</a>] [<a href="CODE">Code</a>] 
            <details><summary>[Bibtex]</summary>
            <div style="background-color:#EEEEEE;">
                BIBTEX
            </div>

            </details>
        </td>
    </tr>
-->
</body>

</html>