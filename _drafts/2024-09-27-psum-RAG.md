---
layout: post
section-type: post
has-comments: false
title: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
category: paper-summary
tags: ["paper-summary","nlp","rag"]
---
# Introduction
LLM's are very cool. There, I said it. Unpopular opinion, I know. They generate very plausible human-like text. The problem is... well, they make things up. All. The. Time. You cannot trust most of what they say. So far, the most I've used them for is to rephrase information I already know. And they are very good at that. And I keep seeing all that super interesting research on agents. You build them to use computers, summarize meetings, prepare your presentations, search the internet for you and summarize findings, play video games... the list goes on. And I would love that we could actually do all those things. The problem is, with LLM's hallucinating constantly, we cannot trust them to do these tasks. I believe the potential aplications, if we managed to remove hallucinations, are endless. 

So, we need further research to ensure that they are not "hallucinatig". However, I do not like this term that much. Saying that when an LLM makes up information is hallucinating it implies that the rest of the time is doing something much more "rational", or at least grounded in reality. "Hallucinations" make no sense, because LLMs are not usually reasoning or giving true information at all. They all just are making next-token predictions on thse spot every time. It just so happens that sometimes they get things right just because of the big statistical engine running inside: "It’s reasonable to assume that one way of being a likely continuation of a text is by being true; if humans are roughly more accurate than chance, true sentences will be more likely than false ones. This might make the chatbot more accurate than chance, but it does not give the chatbot any intention to convey truths." ([Hicks et al.](https://link.springer.com/article/10.1007/s10676-024-09775-5)). 

!!! CHECK HICKS LINK, IT WAS WRONG!!! ^^^^^

Having said that, it does not imply that we cannot achieve models that are better at ensuring that the information given is factual. Promising venues are grounding (give the LLM the ability to highlight sources where its information is coming from), and, crucially and as a first step, the ability to access real, on-line knowledge so it can provide with factual answers. This ability has been coined Retrieval Augmented Generation (RAG), augmenting a traditional LLM with the power to retrieve documents from where it can gather more knowledge.

# Retrieval Augmented Generation
<figure>
    <img src="/assets/img/blog/paper_summaries/rag1.png"
         alt="AltCaption from original paper: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents z-i. For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents." style="border-width: 100px; border-color: white;">
    <figcaption><center><em>Overview of the RAG pipeline. <a href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html">Source</a>. </em></center></figcaption>
</figure>


## Core concept
- Take a pre-trained generative LLM, something that, given a prompt, produces a related text or answer.
- Use an external retriever to fetch relevant documents given the prompt.
- Append those documents together with the prompt before giving it to the LLM.

Apparently this simple idea achieves impressive results, in the sense that it is competitive with task-specific architectures, while being straightforward, can be adapted to new memories at test-time, gaining the ability to update the knowledge at any time, and the retrieved memory is plain text, making it more interpretable than previous works.

They have two versions, one where a single set of documents is used to condition the generation of all tokens int he output (RAG-Sequence) and another where each token can retrieve its own set of relevant documents (RAG-Token). Simply put, RAG-Sequence will test a different output for each retrieved document and selec the most probable, wereas RAG-Token will look at all of them. (This is slightly more nuanced than this)


How is it trained? What's the ground truth?

## Key ideas
- Use of a parametric memory (the actual LLM) and a non-parametric model (a pre-trained differentiable neural retriever to access a "dense vector index of Wikipedia" - [see more about the retrieval part](#annex-i-retrieval)).
- Allows to review and expand memory at test-time, opposed to black box models which would require fine-tunning to expand the knowledge base.

# Results
**It does not stop the model from "hallucinating"**, but having factual information in the prompt definitely helps reducing this effect. In my opinion, this problem goes beyond providing real information, we must find a way for the LLM to access and quote information from the "truth" sources while making sure that the rest of generated output is consistent with that information. Now, I'm no NLP specialist, but that sounds hard.

It seems very competitive with task specific architectures, while being a general tool.

Some cool findings:
- It seems it's capable of retrieving relevant knowledge for jeopardy questions, even when the answer could have multiple answers.
- etc.

# Commentary
I think this work is amazing, and many many people are doing amazing things with it (see [Further reading](#further-reading) below). However, I still believe that the problem is more fundamental than that. If LLMs keep hallucinating even when the true information is given to them as input... we may need to find novel ways for them to handle information so we can make sure that, given a source we trust, the model is able to reformat that information in a way that is the most useful for us while keeping the closest to the truth. I am not really sure if this is somehting we can achieve with plain prompting, or working around the existing technology. We may need to redefine the way in which these models are trained and how they fundamentally treat the data that is given to them.

REPHRASE, ME ESTOY INVENTANDO COSAS, MOLARÍA VER QUÉ COSAS SE HACEN EN GROUNDING QUE IGUAL NO TIENE NADA QUE VER

# Further reading
The idea described above is fairly simple. Modern RAG arquitectures have advanced and further refined the technique. If you want to stay up to date with RAG advancements, I strongly recommend [aishwaryanr's](https://github.com/aishwaryanr/awesome-generative-ai-guide/blob/main/research_updates/rag_research_table.md) and [Timothyxxx's](https://github.com/Timothyxxx/RetrivalLMPapers) repos.

This paper includes relevant citations on the following topics:
- External memories:
	- Parametrized implicit knowledge bases.
	- Retrieval based memories (e.g., REALM and ORQA)
	- Dense passage retriever.
- Memory Networks.
- Stack Augmented Networks.
- Memory Layers.
- Diversity promoting decoding.
- Related work:
	- Single Task Retrieval.
	- General-purpose Architectures for NLP (without retrieval).
	- Learned retrieval (for a specific downstream task).
	- Memory-based architectures.
	- Retrieve-and-edit Approaches.



# Annex I: Retrieval
Traditional (sparse) methods have used an [inverted index](https://www.geeksforgeeks.org/inverted-index/), which fundamentally is a big hash table that, for each word, contains a list of documents that contain it. Several methods such as TF-IDF or BM25 can be used to define which are the relevant terms. However, recent methods use a Transformer that will also select synonyms and other relevant terms that are not necessarily in the query, but related to it.

A dense retriever basically encodes the query/prompt as a deep vector and has all documents similarly encoded. Relevant documents are then retrieved by similarity in a high-dimensional feature space. I guess the advantage here is that you do not need to build a retreival inverse index, just to encode all relevant documents. They will be retrieved by similarity to the whole semantic content, instead of by specific words.

<small>(I learned the difference between sparse and dense methods from [a very detailed reddit response](https://www.reddit.com/r/MachineLearning/comments/z76uel/comment/iy7j1hw/).)</small>

Crucially, the retriever used in this paper has already been trained. It uses MIPS (Maximum Inner Product Search, which apparently [you can solve in sub-linear time](https://arxiv.org/abs/1702.08734)) to compute the highest similarity between the encoded query and the dense vector representation of Wikipedia to retrieve the top-k (in practice, $$$$k=[5,10]$$$$ ) relevant documents. The retriever and document encoder are left fixed (training the document encoder would require to re-encode wikipedia periodically, otherwise the representations would diverge), and only the query encoder and the generator are trained. No supervision is enforced on the retrieved documents, and the job to improve retrieval is left to the query encoder with the signals recieved back from the error on the final generation.

Not sure how they forward the gradients to the query encoder though... The documents are treated as a latent variable and then marginalized so the ouput probabilities are approximated only from the input query (see equations in section 2.1 of the paper). According to the figure I added above, the query is forwarded as is, not as the output of the query encoder. If the output of the query encoder was used to be forwarded to the generator, then it would make sense that it can get backward connections from the generator and use those gradients to be trained. I've spent a couple of hours pulling [this particular thread](https://datascience.stackexchange.com/questions/126514/how-the-retriever-model-query-encoder-is-end-to-end-trained-in-retrieval-augme), and looking at [the code](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L767), but to no avail. I don't want to spend more time on this, so let me know if you find something in this regard.

# Annex II: Beam Search
In my experience with Transformers I've heard time and again that the decoders for NLP tasks are *autoregressive*. What I didn't know is that they do not simply generate the next word in vanilla greedy manner (most probable word at each step). Well, some do, but this is not commonly the case. Most models use a more elaborate form of greedy exploration known as [Beam Search](https://en.wikipedia.org/wiki/Beam_search). Beam search, simply put, is like a BFS algorithm on steroids. It explores the generation tree in a breath first manner, but uses an heuristic to rank the nodes and discards some below a given threshold to maintain a maximum *beam width*. Simply put, is like running the vanilla autoregressive model I had in mind but multiple times until you have produced multiple possible outputs. In the end, the complete output with highest probability is selected (instead of simply individual tokens with highest probabiltiy). This is more costly, but produces better results. See [this](https://www.width.ai/post/what-is-beam-search) for a more detailed description of beam search.

# Annex III: Marginalization
This one's simple, but it's taken me a long while to properly understand it. When I first encountered the term, I jumped to [Wikipedia](https://en.wikipedia.org/wiki/Marginal_distribution) where it's clearly stated:

> Marginal variables are those variables in the subset of variables being retained. These concepts are "marginal" because they can be found by summing values in a table along rows or columns, and writing the sum in the margins of the table.[1] The distribution of the marginal variables (the marginal distribution) is obtained by marginalizing (that is, focusing on the sums in the margin) over the distribution of the variables being discarded, and the discarded variables are said to have been marginalized out. 

But still, I was not understanding it. [This post](https://towardsdatascience.com/probability-concepts-explained-marginalisation-2296846344fc) is what made me understand: Marginalizing over a variable is the result of summing the probabilities over all possible values of that variable, and working with that *marginal distribution*, as if that variable did not exist at all.

 <!-- For the life of me, I spent like an hour trying to write this sentence. Turns out inline math equations in markdown don't like the vertical bar replace it with \mid -->
Now for the real doozie. How does this relate to RAG? Well, instead of working with the joint distribution $$ p(y \mid x,z) $$ , they sum the individual probabilities over all documents $$ z $$ and work with the marginal distribution $$ p(y \mid x) $$ .

To properly understand how they do it we need to take a look at the formulas in the original RAG paper. In the end, what we need is a function that, given a prompt $$ x $$ is able to predict the probability of the possible outputs $$ y $$, so we can select the most probable one. For this they use a BART model. Now, there are two ways in which they marginalize over the retrieved documents $$ z $$: 

- RAG-Sequence:
	$$ p_{\tiny{RAG-Sequence}}(y|x)\; \approx \; \sum_{z \in \text{top-}k(p(\cdot|x))} p_\eta(z|x) p_\theta(y | x, z)\; = \; \sum_{z \in \text{top-}k(p(\cdot|x))} p_\eta(z|x)\prod_i^N p_\theta(y_i|x, z, y_{1:i-1}) $$

- RAG-Token:
	$$ p_{\tiny{RAG-Token}}(y|x) \; \approx \; \prod_{i}^N \; \sum_{z \in \text{top-}k(p(\cdot|x))} p_\eta(z|x) p_\theta(y_i|x, z, y_{1:i-1}) $$




From [this link](https://subirverma.medium.com/retrieval-augmented-generation-deep-dive-8e8db427709f):



