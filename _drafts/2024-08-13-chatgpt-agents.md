---
layout: post
section-type: post
has-comments: false
title: Is ChatGPT actualy AGI?
category: tech
tags: ["tutorial"]
---

From "ChatGPT is bullshit":
- "Frankfurt understands bullshit to be characterized not by an intent to deceive but instead by a reckless disregard for the truth."
- When talking about possible workarounds, and building agents that are able to consult a database etc...
  "“GPT-4 often struggles to formulate a problem in a way that Wolfram Alpha can accept or that produces useful output.” This is not unrelated to the fact that when the language model generates a query for the database or computational module, it does so in the same way it generates text for humans: by estimating the likelihood that some output “looks like’’ the kind of thing the database will correspond with."
- Very interesting point. "Hallucinations" make no sense, because they are not usually reasoning or giving true information at all. They all just are making bullshit on thse spot every tume. It just so happens that sometimes they get things right just because of the big statistical engine running inside: "It’s reasonable to assume that one way of being a likely continuation of a text is by being true; if humans are roughly more accurate than chance, true sentences will be more likely than false ones. This might make the chatbot more accurate than chance, but it does not give the chatbot any intention to convey truths."


It seems an interesting thing would be including the "step-by-step" reasoning within the training itself, may be worth checking.
