<!DOCTYPE html>
<!-- Publications Layout Start --><html lang="en">

  <!-- HEAD Start -->

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Website and blog of Javier Selva.">
  <meta name="author" content="Javier Selva">
  <meta name="keywords" content="selva, javier, personal, academic, ai, research, deep learning, machine learning">
  <link rel="canonical" href="/publications/">
  <title>Javier Selva's Personal Site | Publications</title>

  <!-- CSS links -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.css" rel="stylesheet" integrity="sha512-pli9aKq758PMdsqjNA+Au4CJ7ZatLCCXinnlSfv023z4xmzl8s+Jbj2qNR7RI8DsxFp5e8OvbYGDACzKntZE9w==" crossorigin="anonymous" defer>
  <link href="http://localhost:4000/css/grayscale.css" rel="stylesheet">

  

  

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Maven+Pro:wght@400..900&amp;display=swap" rel="stylesheet" defer>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" rel="stylesheet" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous" defer>

  
    <link rel="shortcut icon" type="image/x-icon" href="http://localhost:4000/assets/img/favicon.ico">
  

  

  

<!-- iOS Web App mode -->

<meta name="apple-mobile-web-app-capable" content="yes">
<link rel="apple-touch-icon" sizes="36x36" href="http://localhost:4000">
<link rel="apple-touch-icon" sizes="48x48" href="http://localhost:4000">
<link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000">
<link rel="apple-touch-icon" sizes="96x96" href="http://localhost:4000">
<link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000">
<link rel="apple-touch-icon" sizes="192x192" href="http://localhost:4000">

<!-- Android Web App mode -->

<link rel="manifest" href="http://localhost:4000/manifest.json">




  <!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">


  
</head>

<!-- HEAD End -->


  <body>
    
<!-- Navigation Start -->

<nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
        <i class="fa fa-bars"></i>
      </button>
      
        <a class="navbar-brand" href="http://localhost:4000/">
      
          <div>
            
              <img src="http://localhost:4000/assets/img/computer.ico" alt="">
            
            Javier Selva Castelló
          </div>
        </a>
    </div>
    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
      <ul class="nav navbar-nav">
        
          <li>
            </li>
<li> <a href="http://localhost:4000/publications/"> Papers </a> </li>
          
        
          <li>
            </li>
<li> <a href="http://localhost:4000/blog/"> Blog </a> </li>
          
        
          <li>
            </li>
<li> <a href="http://localhost:4000/about"> About </a> </li>
          
        
        <li> </li>
        <li>
<!-- Social Buttons Start -->

<ul class="list-inline social-buttons">
  
    
      <li>
        <a href="https://twitter.com/javier_selvac" target="_blank">
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://github.com/javierselva" target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://www.linkedin.com/in/javier-selva-castell%C3%B3-399738134/" target="_blank">
          <i class="fa-brands fa-linkedin-in"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://scholar.google.com/citations?user=T5-DYyUAAAAJ" target="_blank">
          <i class="fa-brands fa-google-scholar"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://www.youtube.com/@javierselva509/featured" target="_blank">
          <i class="fa-brands fa-youtube"></i>
        </a>
      </li>
    
  
</ul>

<!-- Social Buttons End -->
</li>
      </ul> 
    </div>
  </div>
</nav>

<!-- Navigation End -->


    <section id="page" class="container content-section">
      <div class="row">
        <div class="col-md-10 col-md-offset-1 col-xs-10 col-xs-offset-1">
          <h1 id="publications-scholar">Publications (<a href="https://scholar.google.com/citations?user=T5-DYyUAAAAJ" target="_blank">Scholar</a>)</h1>


            
            
              
            
              
            
              
            
              
            
              
                <!-- Post List Start -->
    
    <div style="display: flex;">
      <img src="/assets/img/papers/VTsurvey.png" style="max-width: 100%; height: 100%; flex: 0 0 auto; margin: 0 10px 0 0;">
      <div>
        (TPAMI 2023) 
        <h2><b>Video Transformers: A Survey</b></h2>
<br> 

      </div>

    </div>
     
      
      
        <b> Javier Selva</b>
      
      
       , 
      
     
      
      
        Anders S. Johansen
      
      
       , 
      
     
      
      
        Sergio Escalera
      
      
       , 
      
     
      
      
        Kamal Nasrollahi
      
      
       , 
      
     
      
      
        Thomas B. Moeslund
      
      
       , 
      
     
      
         and 
      
      
        Albert Clapés
      
      
    
    .
    <br> 
    
      
      [<a href="https://ieeexplore.ieee.org/document/10041724" target="blank">Xplore</a>] 
      
    
      
    
      
    
      
      [<a href="https://arxiv.org/abs/2201.05991" target="blank">arXiv</a>] 
      
    
      
    
      
    
    <br>
    <em><p>In this survey, we analyze the main contributions and trends of works leveraging Transformers to model video. Specifically, we delve into how videos are handled at the input level first. Then, we study the architectural changes made to deal with video more efficiently, reduce redundancy, re-introduce useful inductive biases, and capture long-term temporal dynamics. In addition, we provide an overview of different training regimes and explore effective self-supervised learning strategies for video. Finally, we conduct a performance comparison on the most common benchmark for Video Transformers (i.e., action classification), finding them to outperform 3D ConvNets even with less computational complexity.</p>

</em> 
    <details><summary>↓[Bibtex]↓</summary>
    <div style="background-color:#444444;">
      
        
        @article{selva2022video, <br> 
      
        
           
        
        title={Video Transformers: A Survey},  <br> 
      
        
           
        
        author={Selva, Javier and Johansen, Anders S. and Escalera, Sergio and Nasrollahi, Kamal and Moeslund, Thomas B. and Clap{\'e}s, Albert}, <br> 
      
        
           
        
        year={2023}, <br> 
      
        
           
        
        journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},  <br> 
      
        
           
        
        doi={10.1109/TPAMI.2023.3243465} <br> 
      
        
        } <br> 
      
    </div>

    </details>
    <br>
    <hr style="border-color: white; border-width: 1px; margin: 10px 0;">
<!-- Post List End -->
 
              
            
              
                <!-- Post List Start -->
    
    <div style="display: flex;">
      <img src="/assets/img/papers/dyadformer.png" style="max-width: 100%; height: 100%; flex: 0 0 auto; margin: 0 10px 0 0;">
      <div>
        (ICCV 2021 - DYAD Workshop) 
        <h2><b>Dyadformer: A Multi-modal Transformer for Long-Range Modeling of Dyadic Interactions</b></h2>
<br> 

      </div>

    </div>
     
      
      
        David Curto*
      
      
       , 
      
     
      
      
        Albert Clapés*
      
      
       , 
      
     
      
      
        <b> Javier Selva*</b>
      
      
       , 
      
     
      
      
        Sorina Smeureanu
      
      
       , 
      
     
      
      
        Julio C. S. Jacques Junior
      
      
       , 
      
     
      
      
        David Gallardo-Pujol
      
      
       , 
      
     
      
      
        Georgina Guilera
      
      
       , 
      
     
      
      
        David Leiva
      
      
       , 
      
     
      
      
        Thomas B. Moeslund
      
      
       , 
      
     
      
      
        Sergio Escalera
      
      
       , 
      
     
      
         and 
      
      
        Cristina Palmero
      
      
    
    .
    <br> 
    
      
      [<a href="https://openaccess.thecvf.com/content/ICCV2021W/DYAD/papers/Curto_Dyadformer_A_Multi-Modal_Transformer_for_Long-Range_Modeling_of_Dyadic_Interactions_ICCVW_2021_paper.pdf" target="blank">PDF</a>] 
      
    
      
      [<a href="https://openaccess.thecvf.com/content/ICCV2021W/DYAD/supplemental/Curto_Dyadformer_A_Multi-Modal_ICCVW_2021_supplemental.pdf" target="blank">Supp</a>] 
      
    
      
      [<a href="https://arxiv.org/abs/2109.09487" target="blank">arXiv</a>] 
      
    
      
    
      
    
    <br>
    <em><p>We present the Dyadformer, a novel multi-modal multi-subject Transformer architecture to model individual and interpersonal features in dyadic interactions using variable time windows, thus allowing the capture of long-term interdependencies. Our proposed cross-subject layer  allows  the  network  to  explicitly  model  interactions among subjects through attentional operations. This proof-of-concept  approach  shows  how  multi-modality  and  joint modeling  of  both  interactants  for  longer  periods  of  time helps to predict individual attributes.</p>

</em> 
    <details><summary>↓[Bibtex]↓</summary>
    <div style="background-color:#444444;">
      
        
        @InProceedings{Curto_2021_ICCV, <br> 
      
        
           
        
        author    = {Curto, David and Clap{\'e}s, Albert and Selva, Javier and Smeureanu, Sorina and Junior, Julio C. S. Jacques and Gallardo-Pujol, David and Guilera, Georgina and Leiva, David and Moeslund, Thomas B. and Escalera, Sergio and Palmero, Cristina}, <br> 
      
        
           
        
        title     = {Dyadformer: A Multi-Modal Transformer for Long-Range Modeling of Dyadic Interactions}, <br> 
      
        
           
        
        booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops}, <br> 
      
        
           
        
        month     = {October}, <br> 
      
        
           
        
        year      = {2021}, <br> 
      
        
           
        
        pages     = {2177-2188} <br> 
      
        
        } <br> 
      
    </div>

    </details>
    <br>
    <hr style="border-color: white; border-width: 1px; margin: 10px 0;">
<!-- Post List End -->
 
              
            
              
                <!-- Post List Start -->
    
    <div style="display: flex;">
      <img src="/assets/img/papers/udiva.png" style="max-width: 100%; height: 100%; flex: 0 0 auto; margin: 0 10px 0 0;">
      <div>
        (WACV 2021 - HBU Workshop) 
        <h2><b>Context-Aware Personality Inference in Dyadic Scenarios: Introducing the UDIVA Dataset</b></h2>
<br> 

      </div>

    </div>
     
      
      
        Cristina Palmero*
      
      
       , 
      
     
      
      
        <b> Javier Selva*</b>
      
      
       , 
      
     
      
      
        Sorina Smeureanu*
      
      
       , 
      
     
      
      
        Julio C. S. Jacques Junior
      
      
       , 
      
     
      
      
        Albert Clapés
      
      
       , 
      
     
      
      
        Alexa Moseguí
      
      
       , 
      
     
      
      
        Zejian Zhang
      
      
       , 
      
     
      
      
        David Gallardo-Pujol
      
      
       , 
      
     
      
      
        Georgina Guilera
      
      
       , 
      
     
      
      
        David Leiva
      
      
       , 
      
     
      
         and 
      
      
        Sergio Escalera
      
      
    
    .
    <br> 
    
      
      [<a href="https://openaccess.thecvf.com/content/WACV2021W/HBU/papers/Palmero_Context-Aware_Personality_Inference_in_Dyadic_Scenarios_Introducing_the_UDIVA_Dataset_WACVW_2021_paper.pdf" target="blank">PDF</a>] 
      
    
      
      [<a href="https://openaccess.thecvf.com/content/WACV2021W/HBU/supplemental/Palmero_Context-Aware_Personality_Inference_WACVW_2021_supplemental.pdf" target="blank">Supp</a>] 
      
    
      
      [<a href="https://arxiv.org/abs/2012.14259" target="blank">arXiv</a>] 
      
    
      
    
      
      [<a href="https://chalearnlap.cvc.uab.cat/dataset/39/description/" target="blank">Website</a>] 
      
    
    <br>
    <em><p>This paper introduces UDIVA, a new non-acted dataset of face-to-face dyadic interactions, where interlocutors perform competitive and collaborative tasks with different behavior elicitation and cognitive workload. The dataset consists of 90.5 hours of dyadic interactions among 147 participants distributed in 188 sessions, recorded using multiple audiovisual and physiological sensors. Currently, it includes sociodemographic, self- and peer-reported personality, internal state, and relationship profiling from participants.</p>

</em> 
    <details><summary>↓[Bibtex]↓</summary>
    <div style="background-color:#444444;">
      
        
        @inproceedings{palmero2021context, <br> 
      
        
           
        
        title={Context-Aware Personality Inference in Dyadic Scenarios: Introducing the UDIVA Dataset}, <br> 
      
        
           
        
        author={Palmero, Cristina and Selva, Javier and Smeureanu, Sorina and Junior, Julio CS Jacques and Clap{\'e}s, Albert and Mosegu{\'i}, Alexa and Zhang, Zejian and Gallardo-Pujol, David and Guilera, Georgina and Leiva, David and Escalera, Sergio}, <br> 
      
        
           
        
        booktitle={2021 IEEE Winter Conference on Applications of Computer Vision Workshops (WACVW)}, <br> 
      
        
           
        
        pages={1--12}, <br> 
      
        
           
        
        year={2021}, <br> 
      
        
           
        
        organization={IEEE} <br> 
      
        
        } <br> 
      
    </div>

    </details>
    <br>
    <hr style="border-color: white; border-width: 1px; margin: 10px 0;">
<!-- Post List End -->
 
              
            
              
                <!-- Post List Start -->
    
    <div style="display: flex;">
      <img src="/assets/img/papers/recurrent-gaze.png" style="max-width: 100%; height: 100%; flex: 0 0 auto; margin: 0 10px 0 0;">
      <div>
        (BMVC 2018) 
        <h2><b>Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues</b></h2>
<br> 

      </div>

    </div>
     
      
      
        Cristina Palmero
      
      
       , 
      
     
      
      
        <b> Javier Selva</b>
      
      
       , 
      
     
      
      
        Mohammad Ali Bagheri
      
      
       , 
      
     
      
         and 
      
      
        Sergio Escalera
      
      
    
    .
    <br> 
    
      
      [<a href="http://bmvc2018.org/contents/papers/0871.pdf" target="blank">PDF</a>] 
      
    
      
    
      
      [<a href="https://arxiv.org/abs/1805.03064" target="blank">arXiv</a>] 
      
    
      
      [<a href="https://github.com/crisie/RecurrentGaze" target="blank">Code</a>] 
      
    
      
    
    <br>
    <em><p>In this paper, we tackle the problem of person- and head pose-independent 3D gaze estimation from remote cameras, using a multi-modal recurrent convolutional neural network (CNN). We propose to combine face, eyes region, and face landmarks as individual streams in a CNN to estimate gaze in still images. Then, we exploit the dynamic nature of gaze by feeding the learned features of all the frames in a sequence to a many-to-one recurrent module that predicts the 3D gaze vector of the last frame.</p>
</em> 
    <details><summary>↓[Bibtex]↓</summary>
    <div style="background-color:#444444;">
      
        
        @inproceedings{palmero2018recurrent, <br> 
      
        
           
        
        title={Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues}, <br> 
      
        
           
        
        author={Palmero, Cristina and Selva, Javier and Bagheri, Mohammad Ali and Escalera, Sergio}, <br> 
      
        
           
        
        booktitle={Proceedings of the British Machine Vision Conference (BMVC)}, <br> 
      
        
           
        
        year={2018} <br> 
      
        
        } <br> 
      
    </div>

    </details>
    <br>
    <hr style="border-color: white; border-width: 1px; margin: 10px 0;">
<!-- Post List End -->
 
              
            
              
                <!-- Post List Start -->
    
    <div style="display: flex;">
      <img src="/assets/img/papers/frnn.png" style="max-width: 100%; height: 100%; flex: 0 0 auto; margin: 0 10px 0 0;">
      <div>
        (ECCV 2018) 
        <h2><b>Folded Recurrent Neural Networks for Future Video Prediction</b></h2>
<br> 

      </div>

    </div>
     
      
      
        Marc Oliu
      
      
       , 
      
     
      
      
        <b> Javier Selva</b>
      
      
       , 
      
     
      
         and 
      
      
        Sergio Escalera
      
      
    
    .
    <br> 
    
      
      [<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Marc_Oliu_Folded_Recurrent_Neural_ECCV_2018_paper.pdf" target="blank">PDF</a>] 
      
    
      
    
      
      [<a href="https://arxiv.org/abs/1712.00311" target="blank">arXiv</a>] 
      
    
      
      [<a href="https://github.com/moliusimon/frnn" target="blank">Code</a>] 
      
    
      
    
    <br>
    <em><p>This work introduces double-mapping Gated Recurrent Units (dGRU), an extension of standard GRUs where the input is considered as a recurrent state. An extra set of logic gates is added to update the input given the output. Stacking multiple such layers results in a recurrent auto-encoder: the operators updating the outputs comprise the encoder, while the ones updating the inputs from the decoder. Since the states are shared between the corresponding encoder and decoder layers, the representation is stratified during learning: some information is not passed to the next layers.</p>
</em> 
    <details><summary>↓[Bibtex]↓</summary>
    <div style="background-color:#444444;">
      
        
        @inproceedings{oliu2018folded, <br> 
      
        
           
        
        title={Folded recurrent neural networks for future video prediction}, <br> 
      
        
           
        
        author={Oliu, Marc and Selva, Javier and Escalera, Sergio}, <br> 
      
        
           
        
        booktitle={Proceedings of the European Conference on Computer Vision (ECCV)}, <br> 
      
        
           
        
        pages={716--731}, <br> 
      
        
           
        
        year={2018} <br> 
      
        
        } <br> 
      
    </div>

    </details>
    <br>
    <hr style="border-color: white; border-width: 1px; margin: 10px 0;">
<!-- Post List End -->
 
              
            
            

            <!-- Pagination Links Start -->

<div class="pagination">
  <h4>
     

     

    
  </h4>
</div>

<!-- Pagination Links End -->

        </div>
      </div>
    </section>

    <!-- Footer Start -->

<footer>
  <!-- Social Buttons Start -->

<ul class="list-inline social-buttons">
  
    
      <li>
        <a href="https://twitter.com/javier_selvac" target="_blank">
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://github.com/javierselva" target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://www.linkedin.com/in/javier-selva-castell%C3%B3-399738134/" target="_blank">
          <i class="fa-brands fa-linkedin-in"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://scholar.google.com/citations?user=T5-DYyUAAAAJ" target="_blank">
          <i class="fa-brands fa-google-scholar"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://www.youtube.com/@javierselva509/featured" target="_blank">
          <i class="fa-brands fa-youtube"></i>
        </a>
      </li>
    
  
</ul>

<!-- Social Buttons End -->


  <div class="container text-center">
    <p>Copyright © Javier Selva 2024</p>
    <p><a href="https://github.com/le4ker/personal-jekyll-theme/" target="blank">Site's theme.</a></p>
  </div>
</footer>

<!-- Footer End -->
 

    <!-- Javascript Start -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha512-jGsMH83oKe9asCpkOVkBnUrDDTp8wl+adkB2D+//JtlxO4SrLoJdhbOysIFQJloQFD+C4Fl1rMsQZF76JjV0eQ==" crossorigin="anonymous">
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha512-oBTprMeNEKCnqfuqKd6sbvFzmFQtlXS3e0C/RGFV0hD6QzhHV+ODfaQbAlmY6/q0ubbwlAM/nCJjkrgA3waLzg==" crossorigin="anonymous">
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js" integrity="sha512-ahmSZKApTDNd3gVuqL5TQ3MBTj8tL5p2tYV05Xxzcfu6/ecvt1A0j6tfudSGBVuteSoTRMqMljbfdU0g2eDNUA==" crossorigin="anonymous">
</script>

<!--
  * Start Bootstrap - Grayscale Bootstrap Theme (http://startbootstrap.com)
  * Code licensed under the Apache License v2.0.
  * For details, see http://www.apache.org/licenses/LICENSE-2.0.
-->
<script>
  function toggleNavCollapse() {
    50 < $(".navbar").offset().top
      ? $(".navbar-fixed-top").addClass("top-nav-collapse")
      : $(".navbar-fixed-top").removeClass("top-nav-collapse");
  }
  $(document).ready(toggleNavCollapse);
  $(window).scroll(toggleNavCollapse);
  $(function () {
    $("a.page-scroll").bind("click", function (b) {
      var a = $(this);
      $("html, body")
        .stop()
        .animate(
          { scrollTop: $(a.attr("href")).offset().top - 50 },
          1500,
          "easeInOutExpo",
          function () {
            a.blur();
          },
        );
      b.preventDefault();
    });
  });
  $(".navbar-collapse ul li a").click(function () {
    $(".navbar-toggle:visible").click();
  });
</script>





 

   

  



<!-- Javascript End -->



  </body>
</html><!-- Publications Layout End -->
