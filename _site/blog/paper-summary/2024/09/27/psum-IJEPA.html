<!DOCTYPE html>
<!-- Post Layout Start --><html lang="en">
  <!-- HEAD Start -->

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Website and blog of Javier Selva.">
  <meta name="author" content="Javier Selva">
  <meta name="keywords" content="selva, javier, personal, academic, ai, research, deep learning, machine learning">
  <link rel="canonical" href="/blog/paper-summary/2024/09/27/psum-IJEPA.html">
  <title>Javier Selva's Personal Site | I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</title>

  <!-- CSS links -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.css" rel="stylesheet" integrity="sha512-pli9aKq758PMdsqjNA+Au4CJ7ZatLCCXinnlSfv023z4xmzl8s+Jbj2qNR7RI8DsxFp5e8OvbYGDACzKntZE9w==" crossorigin="anonymous" defer>
  <link href="http://localhost:4000/css/grayscale.css" rel="stylesheet">

  

  
    
      <link rel="stylesheet" type="text/css" href="http://localhost:4000/css/everforest.css">
    
    <link rel="stylesheet" href="http://localhost:4000/css/rrssb.css">
  

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Maven+Pro:wght@400..900&amp;display=swap" rel="stylesheet" defer>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" rel="stylesheet" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous" defer>

  
    <link rel="shortcut icon" type="image/x-icon" href="http://localhost:4000/assets/img/favicon.ico">
  

  

  

<!-- iOS Web App mode -->

<meta name="apple-mobile-web-app-capable" content="yes">
<link rel="apple-touch-icon" sizes="36x36" href="http://localhost:4000">
<link rel="apple-touch-icon" sizes="48x48" href="http://localhost:4000">
<link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000">
<link rel="apple-touch-icon" sizes="96x96" href="http://localhost:4000">
<link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000">
<link rel="apple-touch-icon" sizes="192x192" href="http://localhost:4000">

<!-- Android Web App mode -->

<link rel="manifest" href="http://localhost:4000/manifest.json">




  <!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">


  <!-- this is needed to be able to control a little bit what google indexes of the website -->
  <meta name="google-site-verification" content="Lb06aK3u6JCMl7911NwXuKLiqMpBY1umNdJF4sfBgRo">
  
</head>

<!-- HEAD End -->


  <body>
    
<!-- Navigation Start -->

<nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
        <i class="fa fa-bars"></i>
      </button>
      
        <a class="navbar-brand" href="http://localhost:4000/">
      
          <div>
            
              <img src="http://localhost:4000/assets/img/computer.ico" alt="">
            
            Javier Selva Castelló
          </div>
        </a>
    </div>
    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
      <ul class="nav navbar-nav">
        
          <li>
            </li>
<li> <a href="http://localhost:4000/publications/"> Papers </a> </li>
          
        
          <li>
            </li>
<li> <a href="http://localhost:4000/blog/"> Blog </a> </li>
          
        
          <li>
            </li>
<li> <a href="http://localhost:4000/about"> About </a> </li>
          
        
        <li> </li>
        <li>
<!-- Social Buttons Start -->

<ul class="list-inline social-buttons">
  
    
      <li>
        <a href="https://twitter.com/javier_selvac" target="_blank">
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://github.com/javierselva" target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://www.linkedin.com/in/javier-selva-castell%C3%B3-399738134/" target="_blank">
          <i class="fa-brands fa-linkedin-in"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://scholar.google.com/citations?user=T5-DYyUAAAAJ" target="_blank">
          <i class="fa-brands fa-google-scholar"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://www.youtube.com/@javierselva509/featured" target="_blank">
          <i class="fa-brands fa-youtube"></i>
        </a>
      </li>
    
  
</ul>

<!-- Social Buttons End -->
</li>
      </ul> 
    </div>
  </div>
</nav>

<!-- Navigation End -->


    <section id="post" class="container content-section text-center">
      <div class="row">
        <div class="col-md-10 col-md-offset-1">
          <h1><strong>I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</strong></h1>
          <h4>
            <strong>27 Sep 2024</strong>
            <small>
              .
              <a class="category" href="http://localhost:4000/categories/paper-summary.html">
                paper-summary </a>.
              
              <br>
              
              <a class="tag" href="http://localhost:4000/tags/paper-summary.html">#paper-summary</a>
              
            </small>
          </h4>

          <section class="text-justify">
            <h1 id="introduction">Introduction</h1>
<p>When LeCun first published his vision of how an embodied agent should function in the world I was excited. It is always nice to escape briefly into reading a little bit of theories and hypothesis on how things should work. It is important, I believe, to take a step back and make sure we’re still going in the direction we wish to follow, instead of banging our heads blindly against the next engeneering problem.</p>

<p>In his 60 page monograph titled “<a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">A Path Towards Autonomous Machine Intelligence</a>”, LeCun introduces his ideas on what would be required to build an embodied agent into our world. One of the key cornerstones of this proposal are <em>Joint Embedding Predictive Architectures</em> (JEPA). The key idea here is the use of siamese networks (a couple of networks sharing architecture and potentially the weights) so that one receives an input and the other receives a slightly different input (either another part of the same input or a slightly modified version of it). The network is then trained so that the outputs from both of them should be predictible from one another (I recommend reading <a href="https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">this blog post to get the general idea</a> if you don’t feel like reading a 60 page monograph).</p>

<figure>
    <img src="/assets/img/blog/paper_summaries/ijepa0.png" alt="AltCaption from original paper: A diagram explaining the JEPA architecture. The Joint-Embedding Predictive Architecture (JEPA) consists of two encoding branches. The first branch computes sx, a representation of x and the second branch sy a representation of y. The encoders do not need to be identical. A predictor module predicts sy from sx with the possible help of a latent variable z. The energy is the prediction error." style="border-width: 100px; border-color: white;">
    <figcaption><center><em>Joint-Embedding Predictive Architecture (JEPA). <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">Source</a>. </em></center></figcaption>
</figure>

<p>This is far from being something entirely new. Far from it, what LeCun was proposing was a generalization of many different self-supervised learning mechanisms. From <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.pdf">SimSiam (PDF)</a>, to <a href="https://papers.nips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf">BYOL (PDF)</a>, many self-supervised objectives for vision can be cast under this umbrella. Recently, LeCun himself has worked on several papers that implicitly try to be instantiations of the JEPA idea, such as <a href="https://openreview.net/forum?id=xm6YD62D1Ub">VICReg</a> or <a href="http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf">BarlowTwins (PDF)</a>.</p>

<p>Last year we finally go to see the first implementation of this theoretical idea in the form of I-JEPA.</p>

<!-- Maybe an intro to energy based?? It would be helpful for me to fully understand -->

<h1 id="i-jepa">I-JEPA</h1>

<p><a href="https://arxiv.org/abs/2301.08243">I-JEPA</a> has been the first work to explicitly instantiate a JEPA architecture for image self-supervised training. The idea is clear: we want to reconstruct missing parts of the input to force the network to learn relevant patterns in the data in order to solve the task. Now, this is different from the traditional inpainting because of the granularity that transformers provide. As short self-quote from “<a href="https://arxiv.org/abs/2201.05991">Video Transformers: A survey.</a>”:</p>

<blockquote>
  <p>MTM [(Masked Token Modeling)] could be seen from the lens of generative-based pre-training as it bears great resemblance with CNN-based inpainting. We believe that the success of MTM may be attributable to Transformers providing explicit granularity through tokenization. In order to <em>conquer</em> the complex global task of inpainting large missing areas of the input, MTM <em>divides</em> it into smaller local predictions. […] Intuitively, the model needs an understanding of both global appearance […] as well as low-level local patterns to properly gather the necessary context to solve token-wise predictions. This may allow VTs to learn more holistic representations (i.e. better learning of part-whole relationships).</p>
</blockquote>

<p>Also, although it had already been done for Transformers (e.g., MAE or SimMIM), the key novelty with regards to that is that here the predictions are done in feature space (instead of pixel space or HOG features, respectively) by leveraging a siamese network that will produce complete input representations.</p>

<figure>
    <img src="/assets/img/blog/paper_summaries/ijepa1.png" alt="AltCaption from original paper: The Image-based Joint-Embedding Predictive Architecture uses a single context block to predict the representations of various target blocks originating from the same image. The context encoder is a Vision Transformer (ViT), which only processes the visible context patches. The predictor is a narrow ViT that takes the context encoder output and, conditioned on positional tokens (shown in color), predicts the representations of a target block at a specific location. The target representations correspond to the outputs of the target-encoder, the weights of which are updated at each iteration via an exponential moving average of the context encoder weights." style="border-width: 100px; border-color: white;">
    <figcaption><center><em>Image-based Joint-Embedding Predictive Architecture (I-JEPA). <a href="https://arxiv.org/abs/2301.08243">Source</a>. </em></center></figcaption>
</figure>

<!--
- Nice representations for high semantic level, but seem to fail in specific cases (low-level reasoning or unbalanced data): low-level may require different invariances.
	- They show better performance than trying to reconstruct pixels, and seem to be better for low level tasks that other similar methods.

	- Some references that helped me form this insight:
		- Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases
		- Video Representation Learning by Dense Predictive Coding
		- BraVe: Broaden Your Views for Self-Supervised Video Learning
		- Self-Supervised Video Transformer 
		- Efficient Self-supervised Vision Transformers for Representation Learning
		- Learning Representations by Maximizing Mutual Information Across Views
		- 
-->
<h2 id="core-concept">Core concept</h2>
<p>They use two networks, the target encoder and the context encoder (plus a predictor to map representations between the two). From a given input image divided into patches, some of these are selected to be the target. From the remaining bits of the image, multiple blocks are selected to be used as context. The network is tasked with solving a predictive task: from each context block predict the target block.</p>

<figure>
    <img src="/assets/img/blog/paper_summaries/ijepa2.png" alt="Table displaying original images and different possible crops to be used either as target or context." style="border-width: 100px; border-color: white;">
    <figcaption><center><em>Examples for the crops used for target and context. <a href="https://arxiv.org/abs/2301.08243">Source</a>. </em></center></figcaption>
</figure>

<h2 id="key-ideas">Key ideas</h2>
<ul>
  <li>Split the image into tokens. Feed all of them to a <em>target network</em> and produce a contextualized representation for each token. Select some potentially overlaping blocks (groups of tokens) to be defined as a target.</li>
  <li>Next, take the input again and select multiple <em>context blocks</em>. None of the context blocks overlap with the target block, in order to avoid trivial solutions where the network simply forwards input information to the output. Now run individually each context block through the context network to get a representation of that portion of the input.</li>
  <li>Each context’s representation is then fed into a predictor network, conditioned on the target position within the image, which is tasked with reconstructing the target representation given the context.</li>
  <li>To avoid collapse they use asymetries and condition the prediction on the positions that are requested to be predicted (by using as input the PE learned for that position).</li>
  <li>Both networks share the same architecture and initialization, but only the context network is trained through backpropagation (L2 loss), and the target one only updates its weights by an exponential moving average (which seems to be key for these types of settings).</li>
  <li>“JEPAs do not seek representations invariant to a set of hand-crafted data augmentations, but instead seek representations that are predictive of each other when conditioned on additional information z”
    <ul>
      <li>
<strong>No need to use data augmentation</strong>, as they are very expensive (specially in some modalities such as video). This is true, not many methods of this type are brave enough to entirely remove data augmentation from their pipeline, even when the core of their models is these types of syamese architectures. Note that, for me, cropping and resizing <em>is</em> data augmentation. For instance, in <a href="https://arxiv.org/abs/2002.05709">SimCLR</a> a list of data-augmentation operations is present, including crops. Still, it holds that cropping is very cheap compared to other augmentations where you need to alter pixel values (color jittering, black and white, gaussian filters, etc.). Still, it is notable that their model is working with just a few views. I think one key point for this to work is that they are not trying to make representations invariant to perturbations (as contrastive methods do), but to make the representations of partial views of an input predictable from each other.</li>
    </ul>
  </li>
  <li>Instead of learning invariances to specific perturbations, it makes the views predictive of each other, making them context-dependent. In this sense, it is making different parts of the input “<em>aware</em>” of each other by making the output representations be predictable from the others. In other words, it is forcing the representations of a given contextual block to contain enough information to predict the context sorounding it.</li>
  <li>To avoid shorcut solutions, one must mask big portions of the image: “masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block”</li>
</ul>

<h1 id="results">Results</h1>

<figure>
    <img src="/assets/img/blog/paper_summaries/ijepa3.png" alt="Graph showing results for IJEPA and other comparable methods. The graphic shows how IJEPA attains bigger accuracy with less compute time during training than the other reported metods." style="border-width: 100px; border-color: white;">
    <figcaption><center><em>Scaling for I-JEPA show how it converges faster than other methods, specially for bigger models. <a href="https://arxiv.org/abs/2301.08243">Source</a>. </em></center></figcaption>
</figure>

<p>When taking a look at the results one thing kept striking me as odd. Despite the results being very competitive with previous works (both using data augmentation and not), in most cases the core architectures used are different, mostly with regards to the number of parameters. I-JEPA tends to outperform other works, specially when using the large ViT-H, larger input resolution, or smaller patch sizes. The address this claim in the scaling section, by stating that, despite their model being slightly slower than other variants, it was still faster than using data augmentation, and, furthermore, I-JEPA seems to be converging way faster. This is what allowed them to train bigger models, which for other training methods may be unfeasible.</p>

<!--
saying "I-JEPA requires less compute than previous methods and achieves strong performance without relying on handcrafted data-augmentations. Compared to reconstructionbased methods, such as MAE, which directly use pixels as targets, I-JEPA introduces extra overhead by computing targets in representation space (about 7% slower time per iteration). However, since I-JEPA converges in roughly 5⇥ fewer iterations, we still see significant compute savings in practice. Compared to view-invariance based methods, such as iBOT, which rely on hand-crafted data augmentations to create and process multiple views of each image, I-JEPA also runs significantly faster." They argue that, being lighter to train they can afford to go the extra mile in the other direction and make the models bigger, but I am still skeptic unless I see some FLOP stats which they do not report. What is true, however, is that it seems like JEPA does learn faster (i.e., in way less epochs), meaning that a larger model, that in general will require more epochs to converge, is in fact feasibly trainable thanks to I-JEPA. -->

<p>In summary they show competitive results: in linear probing, transfer to other datasets and few-shot linear probing, with the benefit of converging faster. They show specially promising results for low level taks, showing the benefits of using local-global predictive tasks instead of contrastive methods for these tasks. This is further reinforced by MAE being the king in these tasks, a model which actually reconstructs the output at pixel level during training.</p>

<p>It is also interesting to check the final section where they generate the predicted context representations, showing that indeed the model has learned semantically significant features.</p>

<figure>
    <img src="/assets/img/blog/paper_summaries/ijepa4.png" alt="The image shows a table-like structure showing images. Each complete image is accompanied, to the right, by a set of masked out versions of the image, where the missing piece has been filled with a prediction given by the network based on a context block." style="border-width: 100px; border-color: white;">
    <figcaption><center><em>Reconstruction of the target blocks given different context blocks. <a href="https://arxiv.org/abs/2301.08243">Source</a>. </em></center></figcaption>
</figure>

<p><br></p>

<!--	SOME FURTHER NOTES I TOOK ON THE PAPER
	- In my opinion, while they do drop data agumentation to build invariant representations, they still require multi-view (and hence, multiple passes through the network). Still way cheaper, specially for video, to crop and cut than having to modify or apply complex pixel-wise functions to alter the content of the image/video. And I mention this because they explicitly say "Common to these approaches is the need to process multiple usergenerated views of each input image, thereby hindering scalability. By contrast, I-JEPA only requires processing a single view of each image." And in my opinion, cropping is an operation through which you produce views. 

	However, I have some small *semantic* issue with the last paragraph of the related work: They claim "I-JEPA only requires processing a single view of each image". In my opinion, crops are still views of the input. Traditionally, views have been defined as alterations of the original input. For instance, in [SimCLR](https://arxiv.org/abs/2002.05709) some data augmentation functions are define to produce varying *views* of the input. The second such augmentation in Figure 4 of the SimCLR paper is precisely "crop and resize". If you ask me, you still need the views in I-JEPA, which in many cases is going to still be a problem, depending on how sensitive is your model to them. Still, it holds that cropping is very cheap compared to other augmentations where you need to alter pixel values (color jittering, black and white, gaussian filters, etc.). I believe that the thing here is they reverted to multi-crop as the only method to produce views and that still seems to work reasonably well with a few views, different from contrastive approaches that have consistenly shown to necessitate large negative sets. 

	- Minimize information redundancy across embeddings (VICReg, BarlowTwins) vs maximize entropy of average embedding (MSN, DINO... clustering methods). Importantly, both try to maximize invariance to certain data augmentations. The difference is how they avoid collapse. The former wants to enforce that not two features are correlated while the latter force the samples (actually, the cluster centers), to be uniformly distributed (do they?), effectively achieving the same thing. If two of the features were correlated, there would be parts of the space where no sample would be, hence reducing the objective of a uniform distribution.
	- Check!! DINO Says " As shown experimentally in Appendix, centering prevents one dimension to dominate but encourages collapse to the uniform distribution, while the sharpening has the opposite effect". In the MSN paper they claim they are "maximizing entropy", whereas DINO does centering and sharpening: centering makes representations uniform (by subtracting the running mean, salient features will progressively get flattened) whereas sharpening makes them focus on salient features alone (a very low temperature on the softmax will practially put everything to 0 except a few salient features). the balance between the two achieves representations that are different from each other, achieving, I guess, something similar as maximizing entropy: no patterns in representational structure.
	- Although conceptually similar to generative approaches, JEPAs can collapse, hence require some asymetries in the network to avoid them.

	- The networks are ViT-like architectures. This allows for variable number of patches in different runs through the network, meaning that differently-sized context and/or targets can be used. This is also important because they do concatenate learned masked tokens enhanced with positional information to the context tokens, in order to signal the predictor network which patches are to be predicted.

RESULTS
# TOCHECK how many weights do the different vits have??
 - They seem to provide with competitive enough results. I have one problem tough, they make a lot of claims that I-JEPA is better than comparable methods here and there, but in many such occasions that is by means of a larger network, input resolution or smaller patch resolution (which has been shown to help up to a degree [ref]). They argue that, being lighter to train (less augmentation etc.) they can afford to go the extra mile in the other direction and make the models bigger, but I am still sceptic unless I see some FLOP stats which they do not report. What is true, however, is that it seems like JEPA does learn faster (i.e., in less epochs), meaning that a larger model, that in general will require more epochs is in fact feasibly trainable thanks to JEPA... It would seem like invariance learning takes a lot of time. NO!! That is not true!! IJepa does learn faster that other non-augmentation models, but is comparable with data-augmentation-based models.

 - I guess DA is bad because: it is used to learn invariances instead of patterns in the data (?) I guess... but also because it is expensive, no?

 - They perform multiple experiments:
 	- Linear evaluation on Image-Net-1K: I-JEPA seems to outperform most non-data-augmentation methods, except CAE (ref), for which it shows competitive results. When comparing to methods that do use data-augmentation, to me, it is good enough that you can achieve competitive performance with less compute and requiring just a few crop-based views, but in order to outperform data augmentation they need a huge model with higher resolution (or smaller patch size).
 	"Compared to popular methods such as Masked Autoencoders (MAE) [35], Context Autoencoders (CAE) [21], and data2vec [7], which also do not rely on extensive hand-crafted data-augmentations during pretraining, we see that I-JEPA significantly improves linear probing performance, while using less computational effort (see section 7). By leveraging the improved efficiency of I-JEPA, we can train larger models that outperform the best CAE model while using a fraction of the compute. I-JEPA also benefits from scale; in particular, a ViT-H/16 trained at resolution 448 x 448 pixels matches the performance of viewinvariant approaches such as iBOT [75], despite avoiding the use of hand-crafted data-augmentations."
 	- Linear evaluation on Image-Net-1K-1% (only 1% of labeled samples, resulting in around 13 samples per class): These results seem more interesting to me, as when using 12-13 examples per class during the supervised stage, it does seem that this pre-training method does better than the other ones. Still I would rather use the word "competitive" if we look at the L model, it is competitive with data2vec in the sense it requires less epochs (1000 less, which is no small feat), at the expense of 4% accuracy drop; all while outperforming MAE with equivalent model size and still less training). Going to the huge model with smaller patches (more expensive model, makes it tie with data2vec with regards to accuracy, while further reducing the training needs. However the resulting model is going to be more expensive due to the extra flops required. I'd say I rather have a light model that takes more to train (which I am going to do once) than a very expensive model that's cheaper to train. Don't take me wrong, both things are desirable, but if that is the tradeoff when comparing two specific models, I'd say the other wins... But see my next point. Finally, I think it is important to note how the only way to defeat the data augmentation works is through increasing the resolution of the huge model. In this case it is also competitive with regards to the number of epochs, but the fact that these data augmentations are probably using more views (If I recall correctly DINO and friends used around 32 views or such) they are much much more expensive to train. If we look to comparable models, I-JEPA is not really much better (difficult to compare, ViT-L vs ViT-B, but is providing with a method that does not require a super heavy training regime (helping democratize the training of such systems)
 	- Transfer learning to CIFAR100 and Places205: While they still use a huge ViT while some (most) others don't, the differences here are substantial to not possibly be attributed only to model size. This makes the model outperform other non-augmentation dependent methods and very competitive with data-augmentation methods. This generalization abilities also talk good about I-JEPA over other methods. Would be nice to see what they do to try and derive some insights.
 	- Linear probing to object counting and depth prediction: These results make quite a lot of sense. The best performing one (MAE) for these tasks is the one that actually goes full low-level during pre-training and produce a model capable of generating pixels. Followed by I-JEPA, which is trying to preserve low-level information by working at patch level. And the ones that do view invariance, and which are learning high level features perform worse, but not that much worse, mind you... Well... DINO tried to balance local-global by using crops, which could explain why it is better than iBOT even when using a smaller model... Take with a grain of salt, because they are well performing when you consider they are using the base/large architectures where the two best perfroming in this table are using the bigger-more expensive model.

FALTA POR LEER/RESUMIR SECCIONES 7-10



# Future

LeCun has other papers on this topic such as the VICRegL.

Maybe complement this post with the V-JEPA which is a straightforward adaptation to this one, and comment on the limitations for video.

-->

          </section>

          <!-- Share Buttons Start -->
<div>
  <ul class="rrssb-buttons clearfix">
    
      <li class="rrssb-email">
        <a href="mailto:?subject=I-JEPA:%20Self-Supervised%20Learning%20from%20Images%20with%20a%20Joint-Embedding%20Predictive%20Architecture&amp;body=http://localhost:4000http://localhost:4000/blog/paper-summary/2024/09/27/psum-IJEPA.html" data-proofer-ignore>
          <span class="rrssb-icon">
            <svg xmlns="https://www.w3.org/2000/svg" width="28" height="28" viewbox="0 0 28 28">
              <path d="M20.11 26.147c-2.335 1.05-4.36 1.4-7.124 1.4C6.524 27.548.84 22.916.84 15.284.84 7.343 6.602.45 15.4.45c6.854 0 11.8 4.7 11.8 11.252 0 5.684-3.193 9.265-7.398 9.3-1.83 0-3.153-.934-3.347-2.997h-.077c-1.208 1.986-2.96 2.997-5.023 2.997-2.532 0-4.36-1.868-4.36-5.062 0-4.75 3.503-9.07 9.11-9.07 1.713 0 3.7.4 4.6.972l-1.17 7.203c-.387 2.298-.115 3.3 1 3.4 1.674 0 3.774-2.102 3.774-6.58 0-5.06-3.27-8.994-9.304-8.994C9.05 2.87 3.83 7.545 3.83 14.97c0 6.5 4.2 10.2 10 10.202 1.987 0 4.09-.43 5.647-1.245l.634 2.22zM16.647 10.1c-.31-.078-.7-.155-1.207-.155-2.572 0-4.596 2.53-4.596 5.53 0 1.5.7 2.4 1.9 2.4 1.44 0 2.96-1.83 3.31-4.088l.592-3.72z"></path>
            </svg>
          </span>
          <span class="rrssb-text">email</span>
        </a>
      </li>
    

    
      <li class="rrssb-facebook">
        <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000http://localhost:4000/blog/paper-summary/2024/09/27/psum-IJEPA.html&amp;title=I-JEPA:%20Self-Supervised%20Learning%20from%20Images%20with%20a%20Joint-Embedding%20Predictive%20Architecture" class="popup">
          <span class="rrssb-icon">
            <svg xmlns="https://www.w3.org/2000/svg" preserveaspectratio="xMidYMid" width="29" height="29" viewbox="0 0 29 29">
              <path d="M26.4 0H2.6C1.714 0 0 1.715 0 2.6v23.8c0 .884 1.715 2.6 2.6 2.6h12.393V17.988h-3.996v-3.98h3.997v-3.062c0-3.746 2.835-5.97 6.177-5.97 1.6 0 2.444.173 2.845.226v3.792H21.18c-1.817 0-2.156.9-2.156 2.168v2.847h5.045l-.66 3.978h-4.386V29H26.4c.884 0 2.6-1.716 2.6-2.6V2.6c0-.885-1.716-2.6-2.6-2.6z" class="cls-2" fill-rule="evenodd"></path>
            </svg>
          </span>
          <span class="rrssb-text">facebook</span>
        </a>
      </li>
    

    
      <li class="rrssb-twitter">
        <a href="https://twitter.com/share?url=http://localhost:4000http://localhost:4000/blog/paper-summary/2024/09/27/psum-IJEPA.html&amp;text=I-JEPA:%20Self-Supervised%20Learning%20from%20Images%20with%20a%20Joint-Embedding%20Predictive%20Architecture" class="popup">
          <span class="rrssb-icon"><svg xmlns="https://www.w3.org/2000/svg" width="28" height="28" viewbox="0 0 28 28">
              <path d="M24.253 8.756C24.69 17.08 18.297 24.182 9.97 24.62c-3.122.162-6.22-.646-8.86-2.32 2.702.18 5.375-.648 7.507-2.32-2.072-.248-3.818-1.662-4.49-3.64.802.13 1.62.077 2.4-.154-2.482-.466-4.312-2.586-4.412-5.11.688.276 1.426.408 2.168.387-2.135-1.65-2.73-4.62-1.394-6.965C5.574 7.816 9.54 9.84 13.802 10.07c-.842-2.738.694-5.64 3.434-6.48 2.018-.624 4.212.043 5.546 1.682 1.186-.213 2.318-.662 3.33-1.317-.386 1.256-1.248 2.312-2.4 2.942 1.048-.106 2.07-.394 3.02-.85-.458 1.182-1.343 2.15-2.48 2.71z"></path></svg></span>
          <span class="rrssb-text">twitter</span>
        </a>
      </li>
    

    
      <li class="rrssb-linkedin">
        <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:4000http://localhost:4000/blog/paper-summary/2024/09/27/psum-IJEPA.html" class="popup">
          <span class="rrssb-icon">
            <svg xmlns="https://www.w3.org/2000/svg" width="28" height="28" viewbox="0 0 28 28">
              <path d="M25.424 15.887v8.447h-4.896v-7.882c0-1.98-.71-3.33-2.48-3.33-1.354 0-2.158.91-2.514 1.802-.13.315-.162.753-.162 1.194v8.216h-4.9s.067-13.35 0-14.73h4.9v2.087c-.01.017-.023.033-.033.05h.032v-.05c.65-1.002 1.812-2.435 4.414-2.435 3.222 0 5.638 2.106 5.638 6.632zM5.348 2.5c-1.676 0-2.772 1.093-2.772 2.54 0 1.42 1.066 2.538 2.717 2.546h.032c1.71 0 2.77-1.132 2.77-2.546C8.056 3.593 7.02 2.5 5.344 2.5h.005zm-2.48 21.834h4.896V9.604H2.867v14.73z"></path>
            </svg>
          </span>
          <span class="rrssb-text">linkedin</span>
        </a>
      </li>
    

    
      <li class="rrssb-reddit">
        <a href="https://www.reddit.com/submit?url=http://localhost:4000http://localhost:4000/blog/paper-summary/2024/09/27/psum-IJEPA.html&amp;title=I-JEPA:%20Self-Supervised%20Learning%20from%20Images%20with%20a%20Joint-Embedding%20Predictive%20Architecture" target="_blank">
          <span class="rrssb-icon">
            <svg xmlns="https://www.w3.org/2000/svg" width="28" height="28" viewbox="0 0 28 28">
              <path d="M11.794 15.316c0-1.03-.835-1.895-1.866-1.895-1.03 0-1.893.866-1.893 1.896s.863 1.9 1.9 1.9c1.023-.016 1.865-.916 1.865-1.9zM18.1 13.422c-1.03 0-1.895.864-1.895 1.895 0 1 .9 1.9 1.9 1.865 1.03 0 1.87-.836 1.87-1.865-.006-1.017-.875-1.917-1.875-1.895zM17.527 19.79c-.678.68-1.826 1.007-3.514 1.007h-.03c-1.686 0-2.834-.328-3.51-1.005-.264-.265-.693-.265-.958 0-.264.265-.264.7 0 1 .943.9 2.4 1.4 4.5 1.402.005 0 0 0 0 0 .005 0 0 0 0 0 2.066 0 3.527-.46 4.47-1.402.265-.264.265-.693.002-.958-.267-.334-.688-.334-.988-.043z"></path>
              <path d="M27.707 13.267c0-1.785-1.453-3.237-3.236-3.237-.792 0-1.517.287-2.08.76-2.04-1.294-4.647-2.068-7.44-2.218l1.484-4.69 4.062.955c.07 1.4 1.3 2.6 2.7 2.555 1.488 0 2.695-1.208 2.695-2.695C25.88 3.2 24.7 2 23.2 2c-1.06 0-1.98.616-2.42 1.508l-4.633-1.09c-.344-.082-.693.117-.803.454l-1.793 5.7C10.55 8.6 7.7 9.4 5.6 10.75c-.594-.45-1.3-.75-2.1-.72-1.785 0-3.237 1.45-3.237 3.2 0 1.1.6 2.1 1.4 2.69-.04.27-.06.55-.06.83 0 2.3 1.3 4.4 3.7 5.9 2.298 1.5 5.3 2.3 8.6 2.325 3.227 0 6.27-.825 8.57-2.325 2.387-1.56 3.7-3.66 3.7-5.917 0-.26-.016-.514-.05-.768.965-.465 1.577-1.565 1.577-2.698zm-4.52-9.912c.74 0 1.3.6 1.3 1.3 0 .738-.6 1.34-1.34 1.34s-1.343-.602-1.343-1.34c.04-.655.596-1.255 1.396-1.3zM1.646 13.3c0-1.038.845-1.882 1.883-1.882.31 0 .6.1.9.21-1.05.867-1.813 1.86-2.26 2.9-.338-.328-.57-.728-.57-1.26zm20.126 8.27c-2.082 1.357-4.863 2.105-7.83 2.105-2.968 0-5.748-.748-7.83-2.105-1.99-1.3-3.087-3-3.087-4.782 0-1.784 1.097-3.484 3.088-4.784 2.08-1.358 4.86-2.106 7.828-2.106 2.967 0 5.7.7 7.8 2.106 1.99 1.3 3.1 3 3.1 4.784C24.86 18.6 23.8 20.3 21.8 21.57zm4.014-6.97c-.432-1.084-1.19-2.095-2.244-2.977.273-.156.59-.245.928-.245 1.036 0 1.9.8 1.9 1.9-.016.522-.27 1.022-.57 1.327z"></path>
            </svg>
          </span>
          <span class="rrssb-text">reddit</span>
        </a>
      </li>
    

    

    

    

    

    
  </ul>
</div>

<!-- Share Buttons End -->
 <!-- Disqus Comments Start -->


  <div id="disqus_thread"></div>
  <noscript>Please enable JavaScript to view the
    <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
  </noscript>


<!-- Disqus Comments End -->


          <hr>

          
          <div class="author row">
            <img class="col-xs-4 col-sm-3 col-md-2" src="http://localhost:4000/assets/img/me.png" alt="Me">
            <p class="col-xs-8 col-sm-9 col-md-10">Javier Selva has a background of computer science and currently specializes in Machine Learning. In particular he is passionate about NLP, CV and self-supervised learning with Transformers.</p>
          </div>
          
        </div>
      </div>
    </section>

    <!-- Footer Start -->

<footer>
  <!-- Social Buttons Start -->

<ul class="list-inline social-buttons">
  
    
      <li>
        <a href="https://twitter.com/javier_selvac" target="_blank">
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://github.com/javierselva" target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://www.linkedin.com/in/javier-selva-castell%C3%B3-399738134/" target="_blank">
          <i class="fa-brands fa-linkedin-in"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://scholar.google.com/citations?user=T5-DYyUAAAAJ" target="_blank">
          <i class="fa-brands fa-google-scholar"></i>
        </a>
      </li>
    
  
    
      <li>
        <a href="https://www.youtube.com/@javierselva509/featured" target="_blank">
          <i class="fa-brands fa-youtube"></i>
        </a>
      </li>
    
  
</ul>

<!-- Social Buttons End -->


  <div class="container text-center">
    <p>Copyright © Javier Selva 2024</p>
    <p><a href="https://github.com/le4ker/personal-jekyll-theme/" target="blank">Site's theme.</a></p>
  </div>
</footer>

<!-- Footer End -->
 <!-- Javascript Start -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha512-jGsMH83oKe9asCpkOVkBnUrDDTp8wl+adkB2D+//JtlxO4SrLoJdhbOysIFQJloQFD+C4Fl1rMsQZF76JjV0eQ==" crossorigin="anonymous">
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha512-oBTprMeNEKCnqfuqKd6sbvFzmFQtlXS3e0C/RGFV0hD6QzhHV+ODfaQbAlmY6/q0ubbwlAM/nCJjkrgA3waLzg==" crossorigin="anonymous">
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js" integrity="sha512-ahmSZKApTDNd3gVuqL5TQ3MBTj8tL5p2tYV05Xxzcfu6/ecvt1A0j6tfudSGBVuteSoTRMqMljbfdU0g2eDNUA==" crossorigin="anonymous">
</script>

<!--
  * Start Bootstrap - Grayscale Bootstrap Theme (http://startbootstrap.com)
  * Code licensed under the Apache License v2.0.
  * For details, see http://www.apache.org/licenses/LICENSE-2.0.
-->
<script>
  function toggleNavCollapse() {
    50 < $(".navbar").offset().top
      ? $(".navbar-fixed-top").addClass("top-nav-collapse")
      : $(".navbar-fixed-top").removeClass("top-nav-collapse");
  }
  $(document).ready(toggleNavCollapse);
  $(window).scroll(toggleNavCollapse);
  $(function () {
    $("a.page-scroll").bind("click", function (b) {
      var a = $(this);
      $("html, body")
        .stop()
        .animate(
          { scrollTop: $(a.attr("href")).offset().top - 50 },
          1500,
          "easeInOutExpo",
          function () {
            a.blur();
          },
        );
      b.preventDefault();
    });
  });
  $(".navbar-collapse ul li a").click(function () {
    $(".navbar-toggle:visible").click();
  });
</script>





<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<!--
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script” async src=”https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
-->


 

   

  



<!-- Javascript End -->

  </body>
</html><!-- Post Layout End -->
